{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e14984cd-3565-4ef0-a202-9b9d24937a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: mps\n"
     ]
    }
   ],
   "source": [
    "# 03_generator_train.ipynb\n",
    "# Conditional diffusion over wireframe images, conditioned on UI embeddings\n",
    "\n",
    "import os, math, json, random, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils as vutils\n",
    "\n",
    "# device\n",
    "device = \"mps\" if torch.backends.mps.is_available() else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Running on:\", device)\n",
    "\n",
    "# paths\n",
    "SEM_DIR = Path(\"data/rico_semantic_annotations\")      # wireframe-like images live here (your semantic .jpg/.jpeg)\n",
    "EMB_PATHS = [\n",
    "    Path(\"embeddings/semantic_vit_gnn.pkl\"),          # prefer pkl from 02 notebook (fast, robust)\n",
    "    Path(\"embeddings/semantic_vit_gnn.parquet\"),      # if parquet worked for you\n",
    "]\n",
    "\n",
    "# image & train config (keep small to start)\n",
    "IMG_SIZE = 256\n",
    "BATCH_SIZE = 16\n",
    "NUM_WORKERS = 0\n",
    "EPOCHS = 3           # sanity-run; bump later\n",
    "LR = 1e-4\n",
    "SAVE_DIR = Path(\"runs/generator\")\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c7f0417-e6b4-4095-94c1-190755fe9cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditioning dim: 448\n",
      "Usable pairs: 66261\n"
     ]
    }
   ],
   "source": [
    "# load embeddings (pkl recommended; fall back to parquet)\n",
    "if EMB_PATHS[0].exists():\n",
    "    df_emb = pd.read_pickle(EMB_PATHS[0])\n",
    "elif EMB_PATHS[1].exists():\n",
    "    df_emb = pd.read_parquet(EMB_PATHS[1], engine=\"pyarrow\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"No embeddings file found.\")\n",
    "\n",
    "# ensure arrays\n",
    "def to_np(x):\n",
    "    if isinstance(x, list): return np.array(x, dtype=np.float32)\n",
    "    if isinstance(x, np.ndarray): return x.astype(np.float32)\n",
    "    return np.array(x, dtype=np.float32)\n",
    "\n",
    "df_emb[\"vision_emb\"] = df_emb[\"vision_emb\"].apply(to_np)\n",
    "df_emb[\"graph_emb\"]  = df_emb[\"graph_emb\"].apply(to_np)\n",
    "\n",
    "# build {id -> conditioning vector}\n",
    "id_to_cond = {}\n",
    "for _, row in df_emb.iterrows():\n",
    "    emb = np.concatenate([row[\"vision_emb\"], row[\"graph_emb\"]], axis=0)\n",
    "    id_to_cond[str(row[\"id\"])] = emb\n",
    "\n",
    "cond_dim = next(iter(id_to_cond.values())).shape[0]\n",
    "print(\"Conditioning dim:\", cond_dim)\n",
    "\n",
    "# build list of (id, img_path) that exist on disk\n",
    "records = []\n",
    "for sid in id_to_cond.keys():\n",
    "    p = SEM_DIR / f\"{sid}.png\"\n",
    "    if not p.exists():\n",
    "        p = SEM_DIR / f\"{sid}.jpg\"\n",
    "    if p.exists():\n",
    "        records.append((sid, str(p)))\n",
    "print(f\"Usable pairs: {len(records)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6219a01-c8b0-4858-9036-6bf752ccf01f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59634, 6627)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class WireframeDataset(Dataset):\n",
    "    def __init__(self, records, id_to_cond, img_size=256):\n",
    "        self.recs = records\n",
    "        self.id2c = id_to_cond\n",
    "        self.tf = transforms.Compose([\n",
    "            transforms.Resize((img_size, img_size)),\n",
    "            transforms.ToTensor(),                 # [0,1]\n",
    "        ])\n",
    "    def __len__(self):\n",
    "        return len(self.recs)\n",
    "    def __getitem__(self, idx):\n",
    "        sid, img_path = self.recs[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        x = self.tf(img)                           # (3,H,W), range [0,1]\n",
    "        # scale to [-1,1] for diffusion\n",
    "        x = x * 2. - 1.\n",
    "        cond = torch.from_numpy(self.id2c[sid]).float()\n",
    "        return x, cond\n",
    "\n",
    "# split\n",
    "random.shuffle(records)\n",
    "n = len(records)\n",
    "train_recs = records[: int(0.9*n)]\n",
    "val_recs   = records[int(0.9*n):]\n",
    "\n",
    "ds_train = WireframeDataset(train_recs, id_to_cond, IMG_SIZE)\n",
    "ds_val   = WireframeDataset(val_recs, id_to_cond, IMG_SIZE)\n",
    "dl_train = DataLoader(ds_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, drop_last=True)\n",
    "dl_val   = DataLoader(ds_val,   batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, drop_last=False)\n",
    "\n",
    "len(ds_train), len(ds_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bac4a515-14ff-418f-9fd9-47b8f112efbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual convolutional block with FiLM-style conditioning.\n",
    "\n",
    "    Each block performs:\n",
    "        x -> GroupNorm -> Conv -> FiLM (scale + shift)\n",
    "           -> GroupNorm -> Conv -> FiLM\n",
    "           -> skip connection (residual)\n",
    "           -> SiLU activation\n",
    "\n",
    "    FiLM conditioning:\n",
    "        For each conditioning vector c (the fused UI embedding),\n",
    "        we learn a linear projection that outputs gamma/beta (scale/shift)\n",
    "        parameters to modulate the normalization layers.\n",
    "\n",
    "    Args:\n",
    "        in_ch (int): number of input channels\n",
    "        out_ch (int): number of output channels\n",
    "        cond_dim (int): dimension of conditioning vector\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, out_ch, cond_dim):\n",
    "        super().__init__()\n",
    "        # Normalization layers\n",
    "        self.norm1 = nn.GroupNorm(8, in_ch)\n",
    "        self.norm2 = nn.GroupNorm(8, out_ch)\n",
    "\n",
    "        # Convolutions\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1)\n",
    "\n",
    "        # Optional 1√ó1 conv for residual alignment if channels differ\n",
    "        self.skip = nn.Conv2d(in_ch, out_ch, kernel_size=1) if in_ch != out_ch else nn.Identity()\n",
    "\n",
    "        # FiLM modulation: linear layers generate gamma/beta for each norm\n",
    "        self.film1 = nn.Linear(cond_dim, in_ch * 2)   # -> (gamma1, beta1)\n",
    "        self.film2 = nn.Linear(cond_dim, out_ch * 2)  # -> (gamma2, beta2)\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        \"\"\"\n",
    "        Forward pass with FiLM modulation.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): [B, C_in, H, W] input feature map\n",
    "            c (Tensor): [B, cond_dim] conditioning vector\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape [B, C_out, H, W]\n",
    "        \"\"\"\n",
    "        # --- First conv + FiLM ---\n",
    "        g1, b1 = self.film1(c).chunk(2, dim=-1)  # split into gamma, beta\n",
    "        g1, b1 = g1.unsqueeze(-1).unsqueeze(-1), b1.unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "        h = self.norm1(x) * (1 + g1) + b1        # apply FiLM modulation\n",
    "        h = F.silu(self.conv1(h))                # SiLU activation (swish)\n",
    "\n",
    "        # --- Second conv + FiLM ---\n",
    "        g2, b2 = self.film2(c).chunk(2, dim=-1)\n",
    "        g2, b2 = g2.unsqueeze(-1).unsqueeze(-1), b2.unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "        h = self.norm2(h) * (1 + g2) + b2\n",
    "        h = self.conv2(h)\n",
    "\n",
    "        # --- Residual connection ---\n",
    "        out = F.silu(h + self.skip(x))\n",
    "        return out\n",
    "\n",
    "class TinyUNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A minimal U-Net architecture conditioned on UI embeddings.\n",
    "\n",
    "    The U-Net takes an input image x_t (a noisy wireframe)\n",
    "    and predicts the noise to denoise it at each timestep,\n",
    "    conditioned on the latent UI code c.\n",
    "\n",
    "    Architecture outline:\n",
    "        Input (3√óH√óW)\n",
    "          ‚Üì\n",
    "        [Down path]\n",
    "            Conv(3‚Üíbase)\n",
    "            ‚Üì\n",
    "            ConvBlock(base‚Üí2√óbase)\n",
    "            ‚Üì\n",
    "            AvgPool (downsample 2√ó)\n",
    "            ‚Üì\n",
    "            ConvBlock(2√óbase‚Üí4√óbase)\n",
    "            ‚Üì\n",
    "            AvgPool (downsample 2√ó)\n",
    "          ‚Üì\n",
    "        [Bottleneck]\n",
    "            ConvBlock(4√óbase‚Üí4√óbase)\n",
    "          ‚Üì\n",
    "        [Up path]\n",
    "            ConvTranspose(4√óbase‚Üí2√óbase)\n",
    "            Concatenate skip (from 2√óbase)\n",
    "            ConvBlock((2+4)√óbase‚Üí2√óbase)   # after concat: 6√óbase in\n",
    "            ‚Üì\n",
    "            ConvTranspose(2√óbase‚Üíbase)\n",
    "            Concatenate skip (from base)\n",
    "            ConvBlock((1+2)√óbase‚Üíbase)      # after concat: 3√óbase in\n",
    "          ‚Üì\n",
    "        Output\n",
    "            Conv(base‚Üí3)\n",
    "            ‚Üí final RGB prediction\n",
    "\n",
    "    Args:\n",
    "        cond_dim (int): conditioning vector dimension (vision+graph embedding)\n",
    "        base (int): number of base feature channels (default: 32)\n",
    "    \"\"\"\n",
    "    def __init__(self, cond_dim, base=32):\n",
    "        super().__init__()\n",
    "\n",
    "        # --- Encoder (down path) ---\n",
    "        self.inp = nn.Conv2d(3, base, kernel_size=3, padding=1)\n",
    "        self.down1 = ConvBlock(base, base * 2, cond_dim)  # 32‚Üí64\n",
    "        self.pool1 = nn.AvgPool2d(2)                      # 256‚Üí128\n",
    "        self.down2 = ConvBlock(base * 2, base * 4, cond_dim)  # 64‚Üí128\n",
    "        self.pool2 = nn.AvgPool2d(2)                      # 128‚Üí64\n",
    "\n",
    "        # --- Bottleneck ---\n",
    "        self.mid = ConvBlock(base * 4, base * 4, cond_dim)  # 128‚Üí128\n",
    "\n",
    "        # --- Decoder (up path) ---\n",
    "        self.up1 = nn.ConvTranspose2d(base * 4, base * 2, 2, stride=2)\n",
    "        # FIX: input to upblk1 = upsampled(2√óbase) + skip(4√óbase)\n",
    "        self.upblk1 = ConvBlock(base * 2 + base * 4, base * 2, cond_dim)\n",
    "\n",
    "        self.up2 = nn.ConvTranspose2d(base * 2, base, 2, stride=2)\n",
    "        # FIX: input to upblk2 = upsampled(base) + skip(2√óbase)\n",
    "        self.upblk2 = ConvBlock(base + base * 2, base, cond_dim)\n",
    "\n",
    "        # --- Final output ---\n",
    "        self.outp = nn.Conv2d(base, 3, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        \"\"\"\n",
    "        Forward pass through the conditional U-Net.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): [B, 3, H, W] input image (noisy wireframe)\n",
    "            c (Tensor): [B, cond_dim] conditioning embedding\n",
    "\n",
    "        Returns:\n",
    "            [B, 3, H, W] predicted noise (same shape as input)\n",
    "        \"\"\"\n",
    "        # --- Downsampling path ---\n",
    "        x1 = F.silu(self.inp(x))       # shape: [B, base, H, W]\n",
    "        x2 = self.down1(x1, c)         # [B, 2√óbase, H, W]\n",
    "        p1 = self.pool1(x2)            # ‚Üì H/2\n",
    "        x3 = self.down2(p1, c)         # [B, 4√óbase, H/2, W/2]\n",
    "        p2 = self.pool2(x3)            # ‚Üì H/4\n",
    "\n",
    "        # --- Bottleneck ---\n",
    "        m = self.mid(p2, c)            # [B, 4√óbase, H/4, W/4]\n",
    "\n",
    "        # --- Upsampling path ---\n",
    "        u1 = self.up1(m)               # [B, 2√óbase, H/2, W/2]\n",
    "        u1 = torch.cat([u1, x3], dim=1)  # concat skip: [B, (2+4)√óbase, H/2, W/2]\n",
    "        u1 = self.upblk1(u1, c)\n",
    "\n",
    "        u2 = self.up2(u1)              # [B, base, H, W]\n",
    "        u2 = torch.cat([u2, x2], dim=1)  # concat skip: [B, (1+2)√óbase, H, W]\n",
    "        u2 = self.upblk2(u2, c)\n",
    "\n",
    "        # --- Output ---\n",
    "        out = self.outp(u2)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "090e56b1-a5ff-4c81-81a3-8ed626e988e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: torch.Size([2, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "unet = TinyUNet(cond_dim=cond_dim, base=32)\n",
    "x = torch.randn(2, 3, 256, 256)\n",
    "c = torch.randn(2, cond_dim)\n",
    "y = unet(x, c)\n",
    "print(\"Output:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d358f6d7-e734-4c17-a572-144302e0149b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPM(nn.Module):\n",
    "    \"\"\"\n",
    "    Denoising Diffusion Probabilistic Model (DDPM)\n",
    "    ------------------------------------------------\n",
    "    Wraps the conditional U-Net denoiser into a diffusion process\n",
    "    that learns to reverse a fixed Gaussian noise schedule.\n",
    "\n",
    "    At a high level:\n",
    "        q(x_t | x_{t-1}) = N(‚àöŒ±_t * x_{t-1}, Œ≤_t * I)\n",
    "        p_Œ∏(x_{t-1} | x_t, c) = N(Œº_Œ∏(x_t, t, c), œÉ_t^2 * I)\n",
    "\n",
    "    The model learns to predict the noise Œµ added at each step t.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): the denoiser network (TinyUNet)\n",
    "        timesteps (int): number of diffusion steps T\n",
    "        beta_start, beta_end (float): noise schedule parameters\n",
    "    \"\"\"\n",
    "    def __init__(self, model, timesteps=300, beta_start=1e-4, beta_end=0.02):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.T = timesteps\n",
    "\n",
    "        # --- Define linear beta schedule (variance schedule) ---\n",
    "        # Œ≤‚Çú controls how much noise is added at each timestep\n",
    "        betas = torch.linspace(beta_start, beta_end, timesteps)\n",
    "        alphas = 1. - betas\n",
    "        alphas_cumprod = torch.cumprod(alphas, dim=0)  # Œ±ÃÑ‚Çú = ‚àè‚Çõ‚Çå‚ÇÅ·µó Œ±‚Çõ\n",
    "\n",
    "        # --- Register buffers so they are saved on device with model ---\n",
    "        self.register_buffer(\"betas\", betas)\n",
    "        self.register_buffer(\"alphas_cumprod\", alphas_cumprod)\n",
    "        self.register_buffer(\"alphas_cumprod_prev\",\n",
    "            torch.cat([torch.tensor([1.], device=betas.device), alphas_cumprod[:-1]])\n",
    "        )\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Forward diffusion (q) ‚Äî add noise to clean image\n",
    "    # ------------------------------------------------------------------\n",
    "    def q_sample(self, x0, t, noise=None):\n",
    "        \"\"\"\n",
    "        Diffuse the data (add Gaussian noise) for a given timestep t.\n",
    "\n",
    "        Args:\n",
    "            x0 (Tensor): clean image, shape [B, 3, H, W]\n",
    "            t (Tensor): timestep indices, shape [B]\n",
    "            noise (Tensor): optional external noise, same shape as x0\n",
    "\n",
    "        Returns:\n",
    "            x_t (Tensor): noisy image at timestep t\n",
    "            noise (Tensor): the exact noise used (for supervision)\n",
    "        \"\"\"\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x0)\n",
    "\n",
    "        # Gather Œ±ÃÑ‚Çú for each sample in batch\n",
    "        ac = self.alphas_cumprod[t].view(-1, 1, 1, 1)\n",
    "        x_t = torch.sqrt(ac) * x0 + torch.sqrt(1 - ac) * noise\n",
    "        return x_t, noise\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Training objective ‚Äî predict the noise Œµ\n",
    "    # ------------------------------------------------------------------\n",
    "    def p_losses(self, x0, cond, t):\n",
    "        \"\"\"\n",
    "        Compute the DDPM training loss for a given batch.\n",
    "\n",
    "        Steps:\n",
    "            1. Sample random noise Œµ ~ N(0, I)\n",
    "            2. Add it to x0 using q_sample to get x_t\n",
    "            3. Predict ŒµÃÇ = model(x_t, cond)\n",
    "            4. Loss = MSE(ŒµÃÇ, Œµ)\n",
    "\n",
    "        Args:\n",
    "            x0 (Tensor): clean input images\n",
    "            cond (Tensor): conditioning embeddings\n",
    "            t (Tensor): random timestep per sample\n",
    "        \"\"\"\n",
    "        x_t, noise = self.q_sample(x0, t)\n",
    "        noise_pred = self.model(x_t, cond)\n",
    "        return F.mse_loss(noise_pred, noise)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Reverse diffusion step ‚Äî one iteration of sampling\n",
    "    # ------------------------------------------------------------------\n",
    "    @torch.no_grad()\n",
    "    def p_sample(self, x, cond, t):\n",
    "        \"\"\"\n",
    "        Sample x_{t-1} given x_t using the model‚Äôs predicted noise.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): current noisy image [B, 3, H, W]\n",
    "            cond (Tensor): conditioning embeddings\n",
    "            t (Tensor): current timestep index\n",
    "\n",
    "        Returns:\n",
    "            x_prev (Tensor): denoised image for t-1\n",
    "        \"\"\"\n",
    "        betat = self.betas[t].view(-1, 1, 1, 1)\n",
    "        ac = self.alphas_cumprod[t].view(-1, 1, 1, 1)\n",
    "        ac_prev = self.alphas_cumprod_prev[t].view(-1, 1, 1, 1)\n",
    "\n",
    "        # Predict noise ŒµÃÇ = model(x_t, cond)\n",
    "        eps = self.model(x, cond)\n",
    "\n",
    "        # Compute posterior mean Œº_Œ∏(x_t, t, c)\n",
    "        mean = (1 / torch.sqrt(1 - betat)) * (x - betat / torch.sqrt(1 - ac) * eps)\n",
    "\n",
    "        if (t == 0).all():\n",
    "            # last step ‚Äî no noise added\n",
    "            return mean\n",
    "\n",
    "        # Compute variance œÉ‚Çú\n",
    "        z = torch.randn_like(x)\n",
    "        sigma = torch.sqrt((1 - ac_prev) / (1 - ac) * betat)\n",
    "        return mean + sigma * z\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Full sampling loop ‚Äî generate an image from pure noise\n",
    "    # ------------------------------------------------------------------\n",
    "    @torch.no_grad()\n",
    "    def sample(self, cond, shape):\n",
    "        \"\"\"\n",
    "        Generate samples x‚ÇÄ from pure Gaussian noise.\n",
    "\n",
    "        Algorithm:\n",
    "            x_T ~ N(0, I)\n",
    "            for t = T-1 ... 0:\n",
    "                x_t = p_sample(x_{t+1}, t+1)\n",
    "            return x_0\n",
    "\n",
    "        Args:\n",
    "            cond (Tensor): [B, cond_dim] conditioning vectors\n",
    "            shape (tuple): image shape (C, H, W)\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape [B, C, H, W] (denoised samples)\n",
    "        \"\"\"\n",
    "        b = cond.size(0)\n",
    "        x = torch.randn(b, *shape, device=cond.device)\n",
    "        for t in reversed(range(self.T)):\n",
    "            tt = torch.full((b,), t, device=cond.device, dtype=torch.long)\n",
    "            x = self.p_sample(x, cond, tt)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3900a2ca-647e-41bd-a337-ec3efc1764c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------\n",
    "# Training utilities for DDPM model\n",
    "# -------------------------------------------------------------\n",
    "# -------------------------------------------------------------\n",
    "# 1. Utility: save a grid of generated images\n",
    "# -------------------------------------------------------------\n",
    "def save_grid(tensors, path, nrow=8, normalize=True, value_range=(-1, 1)):\n",
    "    \"\"\"\n",
    "    Save a batch of tensors as an image grid.\n",
    "    Args:\n",
    "        tensors: [B, 3, H, W] in range [-1, 1]\n",
    "        path: destination Path for PNG\n",
    "        nrow: number of images per row\n",
    "    \"\"\"\n",
    "    grid = vutils.make_grid(\n",
    "        tensors, nrow=nrow, normalize=normalize, value_range=value_range\n",
    "    )\n",
    "    path.parent.mkdir(exist_ok=True, parents=True)\n",
    "    vutils.save_image(grid, path)\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 2. Utility: one training step\n",
    "# -------------------------------------------------------------\n",
    "def train_one_batch(ddpm, optimizer, x, c, device):\n",
    "    \"\"\"\n",
    "    Perform one gradient step on a batch.\n",
    "    Returns:\n",
    "        scalar loss value\n",
    "    \"\"\"\n",
    "    x = x.to(device)\n",
    "    c = c.to(device)\n",
    "    t = torch.randint(0, ddpm.T, (x.size(0),), device=device, dtype=torch.long)\n",
    "    loss = ddpm.p_losses(x, c, t)\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 3. Utility: sample images for qualitative monitoring\n",
    "# -------------------------------------------------------------\n",
    "@torch.no_grad()\n",
    "def sample_grid(ddpm, cond_batch, img_size, step_name, save_dir, nrow=4):\n",
    "    \"\"\"\n",
    "    Generate a small batch of images for visual progress monitoring.\n",
    "    \"\"\"\n",
    "    ddpm.eval()\n",
    "    cond_batch = cond_batch.to(next(ddpm.parameters()).device)\n",
    "    imgs = ddpm.sample(cond_batch, shape=(3, img_size, img_size))\n",
    "    out_path = Path(save_dir) / f\"samples_{step_name}.png\"\n",
    "    save_grid(imgs, out_path, nrow=nrow)\n",
    "    ddpm.train()\n",
    "    return out_path\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 4. Utility: validate on held-out data\n",
    "# -------------------------------------------------------------\n",
    "@torch.no_grad()\n",
    "def validate(ddpm, dl_val, img_size, save_dir, epoch):\n",
    "    \"\"\"\n",
    "    Generate validation samples at the end of an epoch.\n",
    "    \"\"\"\n",
    "    ddpm.eval()\n",
    "    batch = next(iter(dl_val))\n",
    "    x_val, c_val = batch[0].to(next(ddpm.parameters()).device), batch[1].to(\n",
    "        next(ddpm.parameters()).device\n",
    "    )\n",
    "    x_gen = ddpm.sample(c_val[:8], shape=(3, img_size, img_size))\n",
    "    out_path = Path(save_dir) / f\"samples_epoch{epoch}.png\"\n",
    "    save_grid(x_gen, out_path, nrow=4)\n",
    "    print(f\"‚úÖ Saved validation samples for epoch {epoch} ‚Üí {out_path}\")\n",
    "    ddpm.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49619417-3e64-490e-8268-004f2ea86a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------\n",
    "# 5. Main training function\n",
    "# -------------------------------------------------------------\n",
    "def train_ddpm(\n",
    "    ddpm,\n",
    "    dataloaders,\n",
    "    optimizer,\n",
    "    device,\n",
    "    save_dir,\n",
    "    epochs=10,\n",
    "    img_size=256,\n",
    "    log_every=400,\n",
    "):\n",
    "    \"\"\"\n",
    "    Full training loop for DDPM.\n",
    "\n",
    "    Args:\n",
    "        ddpm: diffusion model (with U-Net inside)\n",
    "        dataloaders: (dl_train, dl_val)\n",
    "        optimizer: optimizer (AdamW)\n",
    "        device: \"mps\" / \"cuda\" / \"cpu\"\n",
    "        save_dir: where to store checkpoints & samples\n",
    "        epochs: number of epochs\n",
    "        img_size: image resolution\n",
    "        log_every: steps between sample generations\n",
    "    \"\"\"\n",
    "    dl_train, dl_val = dataloaders\n",
    "    global_step = 0\n",
    "    save_dir = Path(save_dir)\n",
    "    save_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        ddpm.train()\n",
    "        pbar = tqdm(dl_train, desc=f\"Epoch {epoch}/{epochs}\")\n",
    "        epoch_loss = []\n",
    "\n",
    "        for x, c in pbar:\n",
    "            loss = train_one_batch(ddpm, optimizer, x, c, device)\n",
    "            epoch_loss.append(loss)\n",
    "            global_step += 1\n",
    "            pbar.set_postfix(loss=f\"{loss:.5f}\")\n",
    "\n",
    "            # periodic qualitative sampling\n",
    "            if global_step % log_every == 0:\n",
    "                out_path = sample_grid(ddpm, c[:8], img_size, f\"step{global_step}\", save_dir)\n",
    "                print(f\"üñºÔ∏è Sample saved: {out_path}\")\n",
    "\n",
    "        mean_loss = np.mean(epoch_loss)\n",
    "        print(f\"\\nüìâ Epoch {epoch} mean loss: {mean_loss:.6f}\")\n",
    "\n",
    "        # validation image + checkpoint\n",
    "        validate(ddpm, dl_val, img_size, save_dir, epoch)\n",
    "        ckpt_path = save_dir / f\"ddpm_epoch{epoch}.pt\"\n",
    "        torch.save(ddpm.state_dict(), ckpt_path)\n",
    "        print(f\"üíæ Saved model checkpoint ‚Üí {ckpt_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f952cf4a-2dfa-4fb2-a6f6-c414d21f30ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:   3%|‚ñà‚ñå                                                       | 103/3727 [01:13<43:05,  1.40it/s, loss=0.19841]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m ddpm = DDPM(unet, timesteps=\u001b[32m300\u001b[39m).to(device)\n\u001b[32m      3\u001b[39m opt = torch.optim.AdamW(ddpm.parameters(), lr=LR)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mtrain_ddpm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mddpm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mddpm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdl_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdl_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSAVE_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimg_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mIMG_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_every\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m400\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36mtrain_ddpm\u001b[39m\u001b[34m(ddpm, dataloaders, optimizer, device, save_dir, epochs, img_size, log_every)\u001b[39m\n\u001b[32m     35\u001b[39m epoch_loss = []\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x, c \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     loss = \u001b[43mtrain_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mddpm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m     epoch_loss.append(loss)\n\u001b[32m     40\u001b[39m     global_step += \u001b[32m1\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36mtrain_one_batch\u001b[39m\u001b[34m(ddpm, optimizer, x, c, device)\u001b[39m\n\u001b[32m     37\u001b[39m loss.backward()\n\u001b[32m     38\u001b[39m optimizer.step()\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "unet = TinyUNet(cond_dim=cond_dim, base=32).to(device)\n",
    "ddpm = DDPM(unet, timesteps=300).to(device)\n",
    "opt = torch.optim.AdamW(ddpm.parameters(), lr=LR)\n",
    "\n",
    "train_ddpm(\n",
    "    ddpm=ddpm,\n",
    "    dataloaders=(dl_train, dl_val),\n",
    "    optimizer=opt,\n",
    "    device=device,\n",
    "    save_dir=SAVE_DIR,\n",
    "    epochs=EPOCHS,\n",
    "    img_size=IMG_SIZE,\n",
    "    log_every=400,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f04171-0d03-4216-9f2f-497aa21698bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a17073e-64ab-496e-8cb6-67959adfdec3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
